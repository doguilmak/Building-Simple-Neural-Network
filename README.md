<h1  align=center><font  size = 6>Building Simple Neural Network</font></h1>

<img  src="https://i.giphy.com/media/9N2UvCx7wXLnG/giphy.webp"  height=400  width=1000  alt="https://giphy.com/harvard/"> 

<small>Picture Source:<a  href="https://giphy.com/harvard/">https://giphy.com/harvard/</a></small>

<br>

<h2>Statement</h2>

<p>In this lab, we builded neural networks with Sigmoid, Exponenial ReLU, ReLU and Tanh activation function from scratch and code how it performs predictions using forward propagation. After doing forward propagation, we are going to use backpropagation.</p>

<br>

<h2>Objectives</h2>
<ol>
	<li>Build activation functions.</li>
	<li>Build <i>Artifical Neural Networks (ANNs)</i> from scratch.</li>
	<li>Calculate network output using forward propagation.</li>
	<li>Calculate <i>loss</i> between <i>ground truth</i> and estimated output.</li>
	<li>Update <i>weights</i> and <i>biases</i> throught <i>backpropagation</i>.</li>
	<li>Repeate the above three steps until number of iterations is reached or error between <i>ground truth</i> (<i>T</i>) and <i>predicted output</i> (<i>a<sub>2</sub></i>) is below a predefined threshold.</li>
</ol>  

<br>  

<h2>Keywords</h2>
<ul>
	<li>Computer Science</li>
	<li>Classification</li>
	<li>Activation Functions</li>
	<li>Neural Networks</li>
	<li>Scratch</li>
</ul>

<br>

<h2>Notebooks</h2>

<ol>
	<li>ELU Activation Function: <a href="https://github.com/doguilmak/Building-Simple-Neural-Network/blob/main/ELU_Forward_and_Back_Propagation_ANN.ipynb">ELU_Forward_and_Back_Propagation_ANN.ipynb</a></li>
	<li>Sigmoid Activation Function: <a href="https://github.com/doguilmak/Building-Simple-Neural-Network/blob/main/Sigmoid_Forward_and_Back_Propagation_ANN.ipynb">Sigmoid_Forward_and_Back_Propagation_ANN.ipynb</a></li>
	<li>Tanh Activation Function: <a href="https://github.com/doguilmak/Building-Simple-Neural-Network/blob/main/Tanh_Forward_and_Back_Propagation_ANN.ipynb">Tanh_Forward_and_Back_Propagation_ANN.ipynb</a></li>
	<li>ReLU Activation Function: <a href="https://github.com/doguilmak/Building-Simple-Neural-Network/blob/main/ReLU_Forward_and_Back_Propagation_ANN.ipynb">ReLU_Forward_and_Back_Propagation_ANN.ipynb</a></li>
</ol>

<br>

<h2>Contact Me</h2>

<p>If you have something to say to me please contact me:</p>  

<ul>
	<li>Twitter: <a  href="https://twitter.com/Doguilmak">Doguilmak</a></li>
	<li>Mail address: doguilmak@gmail.com</li>
</ul>
