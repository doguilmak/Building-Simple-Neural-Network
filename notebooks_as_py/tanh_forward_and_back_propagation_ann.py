# -*- coding: utf-8 -*-
"""Tanh_Forward_and_Back_Propagation_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eIj4C3i2Wh62aFBm2SBw9viZbUs-hv9X

<h1 align=center><font size = 6>Building Neural Networks with Two Neurons and Hidden Layers From Scratch</font></h1>

<h1 align=center><font size = 4>Tanh</font></h1>

<img src="https://i.giphy.com/media/9N2UvCx7wXLnG/giphy.webp" height=400 width=900 alt="https://giphy.com/harvard/">

<small>Picture Source:<a href="https://giphy.com/harvard/">https://giphy.com/harvard/</a></small>

<br>

<h2>Introduction</h2>
<p>In this lab, we will build a <i>neural network</i> from scratch and code how it performs predictions using <i>forward propagation</i>. After doing <i>forward propagation</i>, we are going to use <i>backpropagation</i>. We are going to use Tanh for our model. This function is a tangent function. Like the <i>Softmax function</i>, it is similar to the <i>sigmoid activation function</i> in terms of its structure. This function is also known as the <i>Tanh</i> function as mentioned. Itâ€™s non-linear. But unlike <i>sigmoid</i>, its output is zero-centered.</p>

<br>

<h2>Objective:</h2>
<ol>
  <li>Build Tanh function.</li>
  <li>Build <i>Artifical Neural Networks (ANNs)</i> from scratch.</li>
  <li>Calculate network output using forward propagation.</li>
  <li>Calculate <i>loss</i> between <i>ground truth</i> and estimated output.</li>
  <li>Update <i>weights</i> and <i>biases</i> throught <i>backpropagation</i>.</li>
  <li>Repeate the above three steps until number of iterations is reached or error between <i>ground truth</i> (<i>T</i>) and <i>predicted output</i> (<i>a<sub>2</sub></i>) is below a predefined threshold.</li>
</ol>

<br>

<h2>Keywords</h2>
<ul>
  <li>Computer Science</li>
  <li>Classification</li>
  <li>Tanh Function</li>
  <li>Neural Networks</li>
  <li>Scratch</li>
</ul>

<br>

<h2>Table of Contents</h2>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#import">Import Libraries</a></li>
<li><a href="https://#build_neural_network">Building Simple Neural Network</a></li>
<li><a href="https://#build_func">Building Function for Artificial Neural Network with Tanh Function</a></li>
<li><a href="https://#build_func_with_grph">Building Function for Artificial Neural Network with Tanh Function and Loss Graph</a></li>
<br>

<p></p>
Estimated Time Needed: <strong>15 min</strong>
</div>

<br>
<h2 align=center id="import">Import Libraries</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

import matplotlib.pyplot as plt
import numpy as np
from IPython.display import Image
import seaborn as sns

!unzip -q images.zip

"""<br>
<h2 align=center id="build_neural_network">Building Simple Neural Network</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

Image(filename='image_folder/back_prop_.jpg', width=900)

"""<br>
<h3>Creating parameters</h3>
"""

weights = [0.15, 0.45]
biases = [0.40, 0.65]
T = 0.25 #@param {type:"number"}
lr = 0.4 #@param {type:"number"}
threshold = 0.001 #@param {type:"number"}
epochs = 300 #@param {type:"number"}

"""<p>Let's print the weights and biases</p>"""

print(weights)
print(biases)

"""<br>
<h3>Building Tanh Function for the Neural Network</h3>

<h4>Tanh function:</h4>

<br>

$$f(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$

<h4>Derivative of the Tanh function:</h4>

<br>

$$f'(x) = 1 - f(z)^{2}$$
"""

def tanh(x):
    t = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
    dt = 1 - t ** 2
    return t, dt

"""<h4>Weights</h4>"""

print(f'\nw\u2081: {weights[0]}') #  weight 1
print(f'w\u2082: {weights[1]}') #  weight 2

"""<h4>Biases</h4>"""

print(f'\nb\u2081: {biases[0]}') #  bias 1
print(f'b\u2082: {biases[1]}') #  bias 2

"""<p>Now that we have the weights and the biases defined for the network, let's compute the output for a given input x<sub>1</sub>.</p>"""

x_1 = 0.1
print(f"Input of our network: {x_1}")

"""<br>
<h3>Calculate a<sub>2</sub> and Error</h3>

<p>Let's start by computing the wighted sum of the input, z<sub>1</sub> at the first node of the hidden layer.</p>
"""

z_1 = x_1 * weights[0] + biases[0]

print('The weighted sum of the input at the first node in the first hidden layer is {}'.format(z_1))

"""<p>Using a Tanh as the activation function.</p>"""

a_1 = tanh(z_1)[0]
a_1

"""<p>Let's start computing the wighted sum of the input, z<sub>2</sub> at the first node of second hidden layer.</p>"""

z_2 = a_1 * weights[1] + biases[1]

print('The weighted sum of the input at the first node in second hidden layer is {}'.format(z_2))

a_2 = tanh(z_2)[0]
print(f'Model predicted or estimated as {a_2}')

"""<p>Let's calculate error between predicted output (a<sub>2</sub>) and the ground truth (T).</p>"""

E = 1 / 2 * (T - a_2) ** 2
E

"""<br>
<h3>Updating Weights and Biases</h3>

<br>
<h4>Updating w<sub>2</sub></h4>
"""

Image(filename='image_folder/update_w2.png')

weights[1] = weights[1] - lr * (-(T - a_2)) * tanh(a_2)[1] * a_1

print(f'Updated w\u2082 value is {weights[1]}')

"""<br>
<h4>Updating b<sub>2</sub></h4>
"""

Image(filename='image_folder/update_b2.png')

biases[1] = biases[1] - lr * (-(T - a_2)) * tanh(a_2)[1] * 1

print(f'Updated b\u2082 value is {biases[1]}')

"""<br>
<h4>Updating w<sub>1</sub></h4>
"""

Image(filename='image_folder/update_w1.png')

weights[0] = weights[0] - lr * (-(T - a_2)) * tanh(a_2)[1] * weights[1] * tanh(a_1)[1] * x_1

print(f'Updated w\u2081 value is {weights[0]}')

"""<br>
<h4>Updating b<sub>1</sub></h4>
"""

Image(filename='image_folder/update_b1.png')

biases[0] = biases[0] - lr * (-(T - a_2)) * tanh(a_2)[1] * weights[1] * tanh(a_1)[1] * 1

print(f'Updated b\u2081 value is {biases[0]}')

"""<p>Let's print out our updated biases and weights:</p>"""

print(weights)
print(biases)
print(f'\nw\u2081: {weights[0]}') #  weight 1
print(f'w\u2082: {weights[1]}') #  weight 2
print(f'\nb\u2081: {biases[0]}') #  bias 1
print(f'b\u2082: {biases[1]}') #  bias 2

"""<br>
<h2 align=center id="build_func">Building Function for Artificial Neural Network with Tanh Function</h2>

<p>At this stage, a function covering the processes and stages done so far has been created. You are completely free to make changes to the parameters.</p>
"""

def forw_backProp(x_1, w_1, w_2, b_1, b_2, T, lr, treshold, epochs):
    
    i = 0
    while i < epochs:
        
        # Calculate z_1
        z_1 = x_1 * w_1 + b_1
        # Calculate a_1
        a_1 = tanh(z_1)[0]
        # Calculate z_2
        z_2 = a_1 * w_2 + b_2
        # Calculate a_2
        a_2 = tanh(z_2)[0]      
        # Calculate loss between predicted value and the ground truth.
        E = 1 / 2 * (T - a_2) ** 2

        # Updating weights and biases
        w_2 = w_2 - lr * (-(T - a_2)) * tanh(a_2)[1] * a_1
        b_2 = b_2 - lr * (-(T - a_2)) * tanh(a_2)[1] * 1
        w_1 = w_1 - lr * (-(T - a_2)) * tanh(a_2)[1] * w_2 * tanh(a_1)[1] * x_1
        b_1 = b_1 - lr * (-(T - a_2)) * tanh(a_2)[1] * w_2 * tanh(a_1)[1] * 1        
        i+=1
        
        ## print(f'\nw\u2081: {w_1}') #  weight 1
        ## print(f'w\u2082: {w_2}') #  weight 2
        ## print(f'\nb\u2081: {b_1}') #  bias 1
        ## print(f'b\u2082: {b_2}') #  bias 2
        
        print(f'\nEpoch {i} - Loss {E} - Output {a_2}')

forw_backProp(0.1, 0.15, 0.45, 0.4, 0.65, T, lr, threshold, epochs)

"""<br>
<h2 align=center id="build_func_with_grph">Building Function for Artificial Neural Network with Tanh Function and Loss Graph</h2>

"""

def prop_Visual(x_1, w_1, w_2, b_1, b_2, T, lr, treshold, epochs):
    
  def tanh(x):
      t = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
      dt = 1 - t ** 2
      return t, dt

  epoch=[]
  error=[]

  i = 0
  while i < epochs:
      
      # Calculate z_1
      z_1 = x_1 * w_1 + b_1
      # Calculate a_1
      a_1 = tanh(z_1)[0]
      # Calculate z_2
      z_2 = a_1 * w_2 + b_2
      # Calculate a_2
      a_2 = tanh(z_2)[0]     
      # Calculate error between predicted value and the ground truth.
      E = 1 / 2 * (T - a_2) ** 2

      # Updating weights and biases
      w_2 = w_2 - lr * (-(T - a_2)) * tanh(a_2)[1] * a_1
      b_2 = b_2 - lr * (-(T - a_2)) * tanh(a_2)[1] * 1
      w_1 = w_1 - lr * (-(T - a_2)) * tanh(a_2)[1] * w_2 * tanh(a_1)[1] * x_1
      b_1 = b_1 - lr * (-(T - a_2)) * tanh(a_2)[1] * w_2 * tanh(a_1)[1] * 1  
      error.append(E)
      epoch.append(i)        
      i+=1

  plt.figure(figsize=(20, 10))
  sns.set_style('whitegrid')
  plt.title('Loss Graph')
  plt.plot(epoch, error)
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.show()

prop_Visual(0.1, 0.15, 0.45, 0.4, 0.65, T, lr, threshold, epochs)

"""<p>As we can see from the plot, our model working relatively great. 

Tips:</p>

<ol>
  <li>Small treshold : Slow convergence</li>
  <li>Large treshold : Our error may not decrease on every iteration; may not converge.</li>
  <li>Make sure that you are using treshold like <i>0.001</i>, <i>0.01</i>, <i>0.1</i> and etc.</li>
</ol>

<br>

<h1>Contact Me</h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")