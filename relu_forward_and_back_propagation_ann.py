# -*- coding: utf-8 -*-
"""ReLU_Forward_and_Back_Propagation_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14nMyjhOWS_czLbxAAWpzfzf-984grmOf

<h1 align=center><font size = 6>Building Neural Networks with Two Neurons and Hidden Layers From Scratch</font></h1>

<h1 align=center><font size = 4>Rectified Linear Units</font></h1>

<img src="https://i.giphy.com/media/9N2UvCx7wXLnG/giphy.webp" height=400 width=900 alt="https://giphy.com/harvard/">

<small>Picture Source:<a href="https://giphy.com/harvard/">https://giphy.com/harvard/</a></small>

<br>

<h2>Introduction</h2>
<p>In this lab, we will build a neural network from scratch and code how it performs predictions using forward propagation. After doing forward propagation, we are going to use backpropagation. We are going to use ReLU for our model. ReLU, also known as the rectified linear unit function, is another commonly preferred linear activation function. The mathematical representation of the ReLU function is given above. It is a function that takes values greater than or equal to zero, and returns 0 for all values less than zero. It is generally used in CNN applications. Despite its name and appearance, itâ€™s not linear and provides the same benefits as Sigmoid, but with better performance. The advantage of the ReLU function is that there is no vanishing gradient problem. ReLU was developed against the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient</a> problem. Please note that all deep learning libraries have the entire training and prediction processes implemented.</p>

<br>

<br>

<h2>Objective:</h2>
<ol>
  <li>Build ReLU function.</li>
  <li>Build <i>Artifical Neural Networks (ANNs)</i> from scratch.</li>
  <li>Calculate network output using forward propagation.</li>
  <li>Calculate <i>loss</i> between <i>ground truth</i> and estimated output.</li>
  <li>Update <i>weights</i> and <i>biases</i> throught <i>backpropagation</i>.</li>
  <li>Repeate the above three steps until number of iterations is reached or error between <i>ground truth</i> (<i>T</i>) and <i>predicted output</i> (<i>a<sub>2</sub></i>) is below a predefined threshold.</li>
</ol>

<br>

<h2>Keywords</h2>
<ul>
  <li>Computer Science</li>
  <li>Classification</li>
  <li>ReLU Function</li>
  <li>Neural Networks</li>
  <li>Scratch</li>
</ul>

<br>

<h2>Table of Contents</h2>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#import">Import Libraries</a></li>
<li><a href="https://#build_neural_network">Building Simple Neural Network</a></li>
<li><a href="https://#build_func">Building Function for Artificial Neural Network with Sigmoid Function</a></li>
<li><a href="https://#build_func_with_grph">Building Function for Artificial Neural Network with Sigmoid Function and Loss Graph</a></li>
<br>

<p></p>
Estimated Time Needed: <strong>15 min</strong>
</div>

<br>
<h2 align=center id="import">Import Libraries</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

import matplotlib.pyplot as plt
import numpy as np
from IPython.display import Image
import seaborn as sns

!unzip -q images.zip

"""<br>
<h2 align=center id="build_neural_network">Building Simple Neural Network</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

Image(filename='image_folder/back_prop_.jpg', width=900)

"""<br>
<h3>Creating parameters</h3>
"""

weights = [0.15, 0.45]
biases = [0.40, 0.65]
T = 0.25 #@param {type:"number"}
lr = 0.4 #@param {type:"number"}
threshold = 0.001 #@param {type:"number"}
epochs = 300 #@param {type:"number"}

"""<p>Let's print the weights and biases</p>"""

print(weights)
print(biases)

"""<br>
<h3>Building ReLU Function for the Neural Network</h3>

<h4>ReLU function:</h4>

<br>

$$f(x) = 
\begin{cases}
  0 & \text{for $x<0$} \\
  x & \text{for x$\geq$0}
\end{cases}$$
"""

def ReLU(x):
    if x > 0:
        return x
    else:
        return 0

"""<h4>Derivative of the ReLU function:</h4>

$$f'(x) = 
\begin{cases}
  0 & \text{for $x<0$} \\
  1 & \text{for x$\geq$0}
\end{cases}$$
"""

def ReLU_prime(x):
    if x > 0:
        return 1  
    else:
        return 0

"""<h4>Weights</h4>"""

print(f'\nw\u2081: {weights[0]}') #  weight 1
print(f'w\u2082: {weights[1]}') #  weight 2

"""<h4>Biases</h4>"""

print(f'\nb\u2081: {biases[0]}') #  bias 1
print(f'b\u2082: {biases[1]}') #  bias 2

"""<p>Now that we have the weights and the biases defined for the network, let's compute the output for a given input x<sub>1</sub>.</p>"""

x_1 = 0.1
print(f"Input of our network: {x_1}")

"""<br>
<h3>Calculate a<sub>2</sub> and Error</h3>

<p>Let's start by computing the wighted sum of the input, z<sub>1</sub> at the first node of the hidden layer.</p>
"""

z_1 = x_1 * weights[0] + biases[0]

print('The weighted sum of the input at the first node in the first hidden layer is {}'.format(z_1))

"""<p>Using a ReLU as the activation function.</p>"""

a_1 = ReLU(z_1)
a_1

"""<p>Let's start computing the wighted sum of the input, z<sub>2</sub> at the first node of second hidden layer.</p>"""

z_2 = a_1 * weights[1] + biases[1]

print('The weighted sum of the input at the first node in second hidden layer is {}'.format(z_2))

a_2 = ReLU(z_2)
print(f'Model predicted or estimated as {a_2}')

"""<p>Let's calculate error between predicted output (a<sub>2</sub>) and the ground truth (T).</p>"""

E = 1 / 2 * (T - a_2) ** 2
E

"""<br>
<h3>Updating Weights and Biases</h3>

<br>
<h4>Updating w<sub>2</sub></h4>
"""

Image(filename='image_folder/update_w2.png')

weights[1] = weights[1] - lr * (-(T - a_2)) * ReLU_prime(a_2) * a_1

print(f'Updated w\u2082 value is {weights[1]}')

"""<br>
<h4>Updating b<sub>2</sub></h4>
"""

Image(filename='image_folder/update_b2.png')

biases[1] = biases[1] - lr * (-(T - a_2)) * ReLU_prime(a_2) * 1

print(f'Updated b\u2082 value is {biases[1]}')

"""<br>
<h4>Updating w<sub>1</sub></h4>
"""

Image(filename='image_folder/update_w1.png')

weights[0] = weights[0] - lr * (-(T - a_2)) * ReLU_prime(a_2) * weights[1] * ReLU_prime(a_1) * x_1

print(f'Updated w\u2081 value is {weights[0]}')

"""<br>
<h4>Updating b<sub>1</sub></h4>
"""

Image(filename='image_folder/update_b1.png')

biases[0] = biases[0] - lr * (-(T - a_2)) * ReLU_prime(a_2) * weights[1] * ReLU_prime(a_1) * 1

print(f'Updated b\u2081 value is {biases[0]}')

"""<p>Let's print out our updated biases and weights:</p>"""

print(weights)
print(biases)
print(f'\nw\u2081: {weights[0]}') #  weight 1
print(f'w\u2082: {weights[1]}') #  weight 2
print(f'\nb\u2081: {biases[0]}') #  bias 1
print(f'b\u2082: {biases[1]}') #  bias 2

"""<br>
<h2 align=center id="build_func">Building Function for Artificial Neural Network with ReLU Function</h2>

<p>At this stage, a function covering the processes and stages done so far has been created. You are completely free to make changes to the parameters.</p>
"""

def forw_backProp(x_1, w_1, w_2, b_1, b_2, T, lr, treshold, epochs):
    
    i = 0
    while i < epochs:
        
        # Calculate z_1
        z_1 = x_1 * w_1 + b_1
        # Calculate a_1
        a_1 = ReLU(z_1)
        # Calculate z_2
        z_2 = a_1 * w_2 + b_2
        # Calculate a_2
        a_2 = ReLU(z_2)     
        # Calculate loss between predicted value and the ground truth.
        E = 1 / 2 * (T - a_2) ** 2

        # Updating weights and biases
        w_2 = w_2 - lr * (-(T - a_2)) * ReLU_prime(a_2) * a_1
        b_2 = b_2 - lr * (-(T - a_2)) * ReLU_prime(a_2) * 1
        w_1 = w_1 - lr * (-(T - a_2)) * ReLU_prime(a_2) * w_2 * ReLU_prime(a_1) * x_1
        b_1 = b_1 - lr * (-(T - a_2)) * ReLU_prime(a_2) * w_2 * ReLU_prime(a_1) * 1        
        i+=1
        
        ## print(f'\nw\u2081: {w_1}') #  weight 1
        ## print(f'w\u2082: {w_2}') #  weight 2
        ## print(f'\nb\u2081: {b_1}') #  bias 1
        ## print(f'b\u2082: {b_2}') #  bias 2
        
        print(f'\nEpoch {i} - Loss {E} - Output {a_2}')

forw_backProp(0.1, 0.15, 0.45, 0.4, 0.65, T, lr, threshold, epochs)

"""<br>
<h2 align=center id="build_func_with_grph">Building Function for Artificial Neural Network with ReLU Function and Loss Graph</h2>

"""

def prop_Visual(x_1, w_1, w_2, b_1, b_2, T, lr, treshold, epochs):
    
  def ReLU(x):
    if x > 0:
        return x
    else:
        return 0

  def ReLU_prime(x):
    if x > 0:
        return 1  
    else:
        return 0    

  epoch=[]
  error=[]

  i = 0
  while i < epochs:
      
      # Calculate z_1
      z_1 = x_1 * w_1 + b_1
      # Calculate a_1
      a_1 = ReLU(z_1)
      # Calculate z_2
      z_2 = a_1 * w_2 + b_2
      # Calculate a_2
      a_2 = ReLU(z_2)      
      # Calculate error between predicted value and the ground truth.
      E = 1 / 2 * (T - a_2) ** 2

      # Updating weights and biases
      w_2 = w_2 - lr * (-(T - a_2)) * ReLU_prime(a_2) * a_1
      b_2 = b_2 - lr * (-(T - a_2)) * ReLU_prime(a_2) * 1
      w_1 = w_1 - lr * (-(T - a_2)) * ReLU_prime(a_2) * w_2 * ReLU_prime(a_1) * x_1
      b_1 = b_1 - lr * (-(T - a_2)) * ReLU_prime(a_2) * w_2 * ReLU_prime(a_1) * 1   
      error.append(E)
      epoch.append(i)        
      i+=1

  plt.figure(figsize=(20, 10))
  sns.set_style('whitegrid')
  plt.title('Loss Graph')
  plt.plot(epoch, error)
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.show()

prop_Visual(0.1, 0.15, 0.45, 0.4, 0.65, T, lr, threshold, epochs)

"""<p>As we can see from the plot, our model working relatively great. 

Tips:</p>

<ol>
  <li>Small treshold : Slow convergence</li>
  <li>Large treshold : Our error may not decrease on every iteration; may not converge.</li>
  <li>Make sure that you are using treshold like <i>0.001</i>, <i>0.01</i>, <i>0.1</i> and etc.</li>
</ol>

<br>

<h1>Contact Me</h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")